# Project 3: Cloud Data Warehouse
---

## Project Summary

Sparkify is a startup company that has successfuly implemented an on-premise Data Warehouse in the past, satisfying all requirements from analytics team.

As time passed, more users joined Sparkify's streaming music service, leading the company to a more complex analytics and IT infrastructure.

The purpose of this project is to take the company to a whole new level where analyzing a massive amount of data is rapid and simple, and to enable worry-free of infrastructure scalability.

## Cloud Data Warehouse

In order to embrace Sparkify's growth, the data engineering team reassessed the entire data analytics environment and came up with a new design for Cloud Data Warehouse. 

By using Amazon Web Services, Sparkify will be able to focus their attention on the business core. AWS has automated tools to scale and rescale processing resources and storage capacity whenever needed. Also, AWS integration with Big Data tools will allow workflow of massive data on the Cloud. 

For the database design, the star schema approach will be kept for this new strategy. We understand that the star schema has its strenghs and will continue to provide the best analytical enviroment for the company needs. 

## Data Sources

Data resides in two directories that contain files in JSON format:

1. **data/song_data** : Contains metadata about a song and the artist of that song;
2. **data/log_data** : Consists of log files generated by the streaming app based on the songs in the dataset above;

## Data Quality Checks

Analytics are best performed when data follows quality standards, so the following data quality actions were taken in this project:

1. Blank spaces and zeros were replaced to `null`;
2. Duplicate removal on dimension tables. Special mention on users dimension where the most recent user interaction will be picked to be the best version in dim_users, this is to ensure we have the last level status from each user.

## Scripts Usage

1. **etl.py**: Responsible for the orchestration of the entire data flow pipeline that will execute the extraction from JSON source files, transform data with DQ checks and load into Redshift tables;
2. **create_table.py** : Database and tables creation. It is necessary to be run before etl.py so the tables are created and cleaned;
3. **sql_queries.py** : Contains the creation table DDL and inserts DML scripts that will be called by create_table.py. It will drop any existing table before creating it again.
